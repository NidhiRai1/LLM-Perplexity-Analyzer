# LLM-Perplexity-Analyzer

This project appears to align with several key aspects of your outlined requirements:

1. **Loading a Pre-trained Model & Dataset**:
   - The project utilizes the Hugging Face Transformers library to load pre-trained language models, such as GPT-2.
   - It employs the Wikitext-2 dataset from the Datasets library for evaluation purposes.

2. **Computing Perplexity**:
   - The project calculates perplexity scores for a subset of text samples, leveraging existing functions to focus on analysis rather than redefining the metric.

3. **(Optional) Adversarial Testing**:
   - The project explores how minor text modifications, like typos or paraphrasing, impact model performance by recalculating perplexity on these altered texts.

4. **(Optional Extra Credit) Comparative Analysis Across Models**:
   - The project evaluates and compares perplexity scores across multiple pre-trained models, such as GPT-2 and GPT-Neo, to assess their performance differences.

5. **Visualization & Analysis**:
   - The project generates visualizations, including histograms or box plots, to depict the distribution of perplexity scores.
   - It provides concise analyses of the findings, especially when optional tasks are completed, to highlight performance variations.

Overall, the **LLM-Perplexity-Analyzer** project seems to comprehensively address the specified requirements, offering a well-documented Jupyter Notebook that benchmarks pre-trained language models on the Wikitext-2 dataset through perplexity computations and includes optional exploratory analyses. 
